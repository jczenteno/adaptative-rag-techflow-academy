{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langsmith import traceable\n",
    "\n",
    "# Cargar el archivo .env\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
    "from typing import Any as _Any\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed661fa",
   "metadata": {},
   "source": [
    "# Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "qdrant_url = os.getenv('QDRANT_URL')\n",
    "qdrant_key = os.getenv('QDRANT_KEY')\n",
    "collection_name = os.getenv('QDRANT_COLLECTION_NAME')\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(f\"[Qdrant] URL: {qdrant_url} | Collection: {collection_name}\")\n",
    "client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_key\n",
    ")\n",
    "\n",
    "qdrant_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = qdrant_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "message_history_limit = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245eb22",
   "metadata": {},
   "source": [
    "# Conexion con BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from psycopg_pool import ConnectionPool\n",
    "\n",
    "def init_pool(min_size: int = 1, max_size: int = 10) -> ConnectionPool:\n",
    "    host = os.getenv(\"POSTGRES_HOST\")\n",
    "    port = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "    user = os.getenv(\"POSTGRES_USER\")\n",
    "    password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    database = os.getenv(\"POSTGRES_TECHFLOW_DATABASE\")\n",
    "    sslmode = os.getenv(\"POSTGRES_SSLMODE\", \"require\")\n",
    "    \n",
    "    # Construir cadena de conexión (usar dbname en lugar de database)\n",
    "    dsn = f\"host={host} port={port} user={user} password={password} dbname={database} sslmode={sslmode}\"\n",
    "    \n",
    "    _pool = ConnectionPool(dsn, min_size=min_size, max_size=max_size)\n",
    "    return _pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65768005",
   "metadata": {},
   "source": [
    "# Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6e8d6",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749cf85",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.auth import default\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "GOOGLE_SHEETS_SPREADSHEET_ID = os.getenv(\"GOOGLE_SHEETS_SPREADSHEET_ID\")\n",
    "GOOGLE_SHEETS_NAME = os.getenv(\"GOOGLE_SHEETS_NAME\")\n",
    "GOOGLE_CREDENTIALS_FILE = os.getenv(\"GOOGLE_CREDENTIALS_FILE\")\n",
    "\n",
    "def registrar_cliente(email: str, nombres: str, apellidos: str, numero_documento: str, telefono: str):\n",
    "    \"\"\"\n",
    "    Registra un nuevo cliente\n",
    "\n",
    "    Args:\n",
    "        email: Correo electrónico del usuario\n",
    "        nombres: Nombre del usuario\n",
    "        apellidos: Apellido del usuario\n",
    "        numero_documento: Número de documento\n",
    "        telefono: Número de teléfono\n",
    "\n",
    "    Returns:\n",
    "        str: 'ok' si la operación fue exitosa\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configuración de credenciales\n",
    "        credentials_file = GOOGLE_CREDENTIALS_FILE\n",
    "        spreadsheet_id = GOOGLE_SHEETS_SPREADSHEET_ID\n",
    "        sheet_name = GOOGLE_SHEETS_NAME\n",
    "\n",
    "        # Scopes necesarios\n",
    "        scopes = [\n",
    "            'https://www.googleapis.com/auth/spreadsheets',\n",
    "            'https://www.googleapis.com/auth/drive.file'\n",
    "        ]\n",
    "\n",
    "        # Autenticación\n",
    "        if credentials_file and os.path.exists(credentials_file):\n",
    "            # Local: usar archivo JSON\n",
    "            credentials = Credentials.from_service_account_file(credentials_file, scopes=scopes)\n",
    "            print(\"Usando archivo JSON local\")\n",
    "        else:\n",
    "            # Cloud Run: usar Application Default Credentials\n",
    "            credentials, _ = default(scopes=scopes)\n",
    "            \n",
    "        print(\"Usando Application Default Credentials\")\n",
    "        #credentials = Credentials.from_service_account_file(scopes=scopes)\n",
    "        service = build('sheets', 'v4', credentials=credentials)\n",
    "\n",
    "        fecha_registro = datetime.now().strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "        # Preparar los datos de la nueva fila\n",
    "        new_row = [\n",
    "            fecha_registro,\n",
    "            email,\n",
    "            nombres,\n",
    "            apellidos,\n",
    "            numero_documento,\n",
    "            telefono\n",
    "        ]\n",
    "\n",
    "        # Rango donde agregar la nueva fila\n",
    "        range_to_append = f\"{sheet_name}!A:F\"\n",
    "\n",
    "        # Cuerpo de la petición\n",
    "        body = {\n",
    "            'values': [new_row]\n",
    "        }\n",
    "\n",
    "        # Encontrar la próxima fila vacía para evitar copiar formato\n",
    "        # Primero obtenemos los datos existentes para saber cuántas filas hay\n",
    "        existing_data = service.spreadsheets().values().get(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=f\"{sheet_name}!A:A\"\n",
    "        ).execute()\n",
    "\n",
    "        # Calcular la próxima fila vacía\n",
    "        existing_rows = len(existing_data.get('values', []))\n",
    "        next_row = existing_rows + 1\n",
    "\n",
    "        # Insertar en la fila específica (esto evita copiar formato)\n",
    "        specific_range = f\"{sheet_name}!A{next_row}:F{next_row}\"\n",
    "\n",
    "        result = service.spreadsheets().values().update(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=specific_range,\n",
    "            valueInputOption='RAW',\n",
    "            body=body\n",
    "        ).execute()\n",
    "\n",
    "        rows_added = result.get(\"updates\", {}).get(\"updatedRows\", 0)\n",
    "        print(f'Usuario registrado correctamente')\n",
    "        print(f'Fecha de registro: {fecha_registro}')\n",
    "\n",
    "        return 'ok'\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f'Error al registrar el usuario: {error}')\n",
    "        raise error\n",
    "\n",
    "def contar_registros():\n",
    "    \"\"\"\n",
    "    Cuenta cuántos registros (filas) hay en el Google Sheet\n",
    "\n",
    "    Returns:\n",
    "        str: Mensaje con el número de registros\n",
    "    \"\"\"\n",
    "    include_headers = False\n",
    "\n",
    "    # Configuración de credenciales\n",
    "    credentials_file = GOOGLE_CREDENTIALS_FILE\n",
    "    spreadsheet_id = GOOGLE_SHEETS_SPREADSHEET_ID\n",
    "    sheet_name = GOOGLE_SHEETS_NAME\n",
    "    \n",
    "    try:\n",
    "        # Scopes necesarios para Google Sheets\n",
    "        scopes = [\n",
    "            'https://www.googleapis.com/auth/spreadsheets',\n",
    "            'https://www.googleapis.com/auth/drive.file'\n",
    "        ]\n",
    "\n",
    "        # Autenticación\n",
    "        if credentials_file and os.path.exists(credentials_file):\n",
    "            # Local: usar archivo JSON\n",
    "            credentials = Credentials.from_service_account_file(credentials_file, scopes=scopes)\n",
    "            print(\"Usando archivo JSON local\")\n",
    "        else:\n",
    "            # Cloud Run: usar Application Default Credentials\n",
    "            credentials, _ = default(scopes=scopes)\n",
    "\n",
    "        # Crear el servicio de Google Sheets\n",
    "        service = build('sheets', 'v4', credentials=credentials)\n",
    "\n",
    "        # Obtener todas las filas con datos (columna A como referencia)\n",
    "        result = service.spreadsheets().values().get(\n",
    "            spreadsheetId=GOOGLE_SHEETS_SPREADSHEET_ID,\n",
    "            range=f\"{GOOGLE_SHEETS_NAME}!A:A\"\n",
    "        ).execute()\n",
    "\n",
    "        # Contar filas con datos\n",
    "        rows_with_data = result.get('values', [])\n",
    "        total_rows = len(rows_with_data)\n",
    "\n",
    "        if include_headers:\n",
    "            registros = total_rows\n",
    "            print(f'Total de filas (incluyendo headers): {registros}')\n",
    "        else:\n",
    "            # Restar 1 para excluir la fila de headers\n",
    "            registros = max(0, total_rows - 1)\n",
    "            print(f'Total de registros (sin headers): {registros}')\n",
    "            print(f'Total de filas (con headers): {total_rows}')\n",
    "\n",
    "        return f\"Hay {registros} clientes registrados\"\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f'Error al contar registros: {error}')\n",
    "        raise error\n",
    "\n",
    "def get_current_date() -> dict:\n",
    "    \"\"\"\n",
    "    Obtener la fecha actual en el formato YYYY-MM-DD\n",
    "    \"\"\"\n",
    "    return {\"current_date\": datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "def get_program_price(program_name: str) -> dict:\n",
    "    \"\"\"Obtiene el precio de un programa mediante su nombre.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    program_name: str\n",
    "        Nombre del programa a buscar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\"program_price\": float | None}\n",
    "    \"\"\"\n",
    "    pool = init_pool()\n",
    "    print(f\"Buscando precio del programa: {program_name}\")\n",
    "\n",
    "    query = (\n",
    "        \"\"\"\n",
    "        SELECT precio\n",
    "        FROM public.programas\n",
    "        WHERE nombre ILIKE '%%' || %s || '%%'\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with pool.connection() as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(query, (program_name,))\n",
    "                row: Optional[tuple] = cursor.fetchone()\n",
    "                if not row:\n",
    "                    return {\"program_price\": None}\n",
    "                price_value = row[0]\n",
    "                try:\n",
    "                    # Asegurar retorno numérico float\n",
    "                    price_float = float(price_value) if price_value is not None else None\n",
    "                except Exception:\n",
    "                    price_float = None\n",
    "                print(f\"Precio del programa: {price_float}\")\n",
    "                return {\"program_price\": price_float}\n",
    "    \n",
    "\n",
    "    except Exception as exc:\n",
    "        # Log simple; en entornos reales usar logger\n",
    "        print(f\"[get_program_price] Error consultando precio: {exc}\")\n",
    "        return {\"program_price\": None}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805229b4",
   "metadata": {},
   "source": [
    "### Prueba de tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_program_price(\"Data architect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c37cb8",
   "metadata": {},
   "source": [
    "## Model con tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.bind_tools([\n",
    "    registrar_cliente,\n",
    "    contar_registros,\n",
    "    get_current_date,\n",
    "    get_program_price,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "resp_tool = model.invoke(\"cual es el costo del curso Arquitectura de Datos Empresarial?\")\n",
    "print(resp_tool.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86718c6",
   "metadata": {},
   "source": [
    "## Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Estado personalizado que incluye mensajes y metadatos del RAG adaptativo\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    complexity_level: Optional[str]\n",
    "    retrieved_docs: Optional[List[Any]]\n",
    "    question: Optional[str]  # Pregunta actual (original o reformulada)\n",
    "    generation: Optional[str]  # Respuesta generada\n",
    "    answer_evaluation: Optional[str]  # Resultado de evaluación de la respuesta ('yes'/'no')\n",
    "    max_retries: Optional[int]  # Máximo número de reintentos\n",
    "    retry_count: Optional[int]  # Contador de reintentos\n",
    "    rag_processs: Optional[str]  # Proceso de RAG ('rag'/'no_rag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f19ac1",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEXITY_PROMPT = \"\"\"Eres un experto clasificando la complejidad de preguntas de estudiantes de TechFlow Academy.\n",
    "\n",
    "Clasifica la pregunta en uno de estos 3 niveles:\n",
    "\n",
    "SIMPLE: Saludos, consultas básicas sobre horarios, ubicación, contacto, despedidas\n",
    "- Ejemplos: \"Hola\", \"¿Están abiertos?\", \"¿Dónde quedan?\", \"Gracias\", \"Adiós\"\n",
    "\n",
    "RAG: Preguntas sobre programas, cursos, profesores, metodología, contenido académico\n",
    "- Ejemplos: \"¿Qué incluye Data Engineering?\", \"¿Quién enseña ML?\", \"¿Cómo son las clases?\"\n",
    "\n",
    "TOOLS: Preguntas sobre costos, inscripciones, disponibilidad de cupos, registro\n",
    "- Ejemplos: \"¿Cuánto cuesta?\", \"¿Hay cupos en Data Science?\", \"Quiero inscribirme\"\n",
    "\n",
    "Retorna JSON con clave 'complexity_level' y valor 'simple', 'rag' o 'tools'.\n",
    "\n",
    "Pregunta: {question}\"\"\"\n",
    "\n",
    "TOOL_ROUTER_PROMPT = \"\"\"Eres un router que determina qué herramienta usar según la pregunta del estudiante.\n",
    "\n",
    "Herramientas disponibles:\n",
    "- get_course_cost: Para preguntas sobre precios, costos, valores de programas\n",
    "- get_student_count: Para preguntas sobre cuántos estudiantes, disponibilidad, cupos\n",
    "- register_student: Para inscripciones, registros, matriculas\n",
    "\n",
    "Ejemplos:\n",
    "\"¿Cuánto cuesta Data Engineering?\" → get_course_cost\n",
    "\"¿Hay cupos disponibles?\" → get_student_count  \n",
    "\"Quiero inscribirme en ML Engineer\" → register_student\n",
    "\n",
    "Retorna JSON con clave 'tool_name' y el nombre de la herramienta.\n",
    "\n",
    "Pregunta: {question}\"\"\"\n",
    "\n",
    "SIMPLE_PROMPT = \"\"\"Eres un asistente amigable de TechFlow Academy, un instituto de programación y ciencia de datos en Lima, Perú.\n",
    "\n",
    "Para consultas simples como saludos, ubicación, horarios básicos, contacto:\n",
    "- Responde de forma cordial y directa\n",
    "- Menciona que TechFlow Academy es especialista en Data Engineering, ML Engineering, Data Visualization, etc.\n",
    "- Para consultas específicas sobre programas, costos o inscripciones, indica que puedes ayudar con información detallada\n",
    "- Mantén un tono profesional pero cercano\n",
    "\n",
    "Información básica:\n",
    "- Horarios: Lunes a viernes 7AM-10PM, Sábados 8AM-8PM\n",
    "- Sedes: Miraflores, San Isidro, La Molina, Surco\n",
    "- Modalidades: Virtual, Presencial, Híbrida\n",
    "- WhatsApp: Canal principal de comunicación\"\"\"\n",
    "\n",
    "RAG_PROMPT = \"\"\"Eres un asistente especializado de TechFlow Academy. \n",
    "\n",
    "Responde la pregunta basándote únicamente en el contexto proporcionado sobre nuestros programas, profesores, metodología y servicios.\n",
    "\n",
    "Instrucciones:\n",
    "- Usa solo la información del contexto\n",
    "- Si no encuentras información específica, indica que puedes ayudar de otra manera\n",
    "- Mantén respuestas claras y estructuradas\n",
    "- Incluye detalles relevantes como duración, modalidades, requisitos\n",
    "- Sugiere próximos pasos cuando sea apropiado (ej: \"¿Te gustaría conocer los costos?\")\"\"\"\n",
    "\n",
    "TOOL_PROMPT = \"\"\"Eres un asistente de TechFlow Academy especializado en información sobre costos, inscripciones y disponibilidad.\n",
    "\n",
    "Genera una respuesta natural y útil basada en el resultado de la herramienta consultada.\n",
    "\n",
    "Instrucciones:\n",
    "- Presenta la información de forma clara y estructurada\n",
    "- Incluye próximos pasos o acciones recomendadas\n",
    "- Mantén tono profesional pero amigable\n",
    "- Si es información de costos, menciona opciones de financiamiento\n",
    "- Si es sobre disponibilidad, sugiere alternativas si es necesario\n",
    "- Si es registro, confirma y explica siguientes pasos\"\"\"\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"Eres un asistente especializado de TechFlow Academy, un instituto de programación y ciencia de datos en Lima, Perú.\n",
    "\n",
    "Tu función es ayudar a estudiantes potenciales y actuales con información sobre programas, inscripciones y servicios.\n",
    "\n",
    "Instrucciones:\n",
    "- Mantén respuestas claras y profesionales\n",
    "- Usa las herramientas disponibles cuando sea necesario para obtener información actualizada\n",
    "- Si necesitas registrar un estudiante, asegúrate de obtener todos los datos requeridos\n",
    "- Para consultas sobre costos o cupos, usa las herramientas correspondientes\n",
    "- Proporciona información útil y sugiere próximos pasos cuando sea apropiado\n",
    "\n",
    "Herramientas disponibles:\n",
    "- registrar_cliente: Para registrar información de estudiantes interesados\n",
    "- contar_registros: Para consultar cuántos estudiantes están registrados\n",
    "- get_current_date: Para obtener la fecha actual\n",
    "\n",
    "Información básica de TechFlow Academy:\n",
    "- Horarios: Lunes a viernes 7AM-10PM, Sábados 8AM-8PM\n",
    "- Sedes: Miraflores, San Isidro, La Molina, Surco\n",
    "- Modalidades: Virtual, Presencial, Híbrida\n",
    "- Programas: Data Engineering, ML Engineering, Data Visualization, Data Science\"\"\"\n",
    "\n",
    "# Prompts para RAG Adaptativo\n",
    "\n",
    "GRADE_DOCUMENTS_PROMPT = \"\"\"Eres un evaluador que determina si los documentos recuperados son relevantes para responder la pregunta del estudiante.\n",
    "\n",
    "Evalúa cada documento y determina si contiene información útil para responder la pregunta.\n",
    "\n",
    "Documentos relevantes son aquellos que:\n",
    "- Contienen información directamente relacionada con la pregunta\n",
    "- Proporcionan contexto útil para formular una respuesta completa\n",
    "- Incluyen detalles específicos sobre programas, profesores, metodología, etc.\n",
    "\n",
    "Documentos NO relevantes son aquellos que:\n",
    "- No tienen relación con la pregunta\n",
    "- Contienen información genérica sin valor específico\n",
    "- No aportan contexto útil para la respuesta\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Documentos a evaluar: {documents}\n",
    "\n",
    "Responde con 'yes' si al menos uno de los documentos es relevante, o 'no' si ninguno es relevante.\"\"\"\n",
    "\n",
    "EVALUATE_ANSWER_PROMPT = \"\"\"Eres un evaluador que determina si una respuesta generada responde adecuadamente la pregunta del estudiante.\n",
    "\n",
    "Evalúa si la respuesta:\n",
    "- Responde directamente la pregunta formulada\n",
    "- Proporciona información específica y útil\n",
    "- Está basada en el contexto proporcionado\n",
    "- No contiene información inventada o alucinada\n",
    "- Es clara y comprensible\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta generada: {generation}\n",
    "\n",
    "Documentos de contexto: {documents}\n",
    "\n",
    "Responde con 'yes' si la respuesta es adecuada, o 'no' si necesita mejoras.\"\"\"\n",
    "\n",
    "REWRITE_QUESTION_PROMPT = \"\"\"Eres un experto en reformular preguntas para mejorar la recuperación de documentos relevantes.\n",
    "\n",
    "La pregunta original no obtuvo documentos relevantes. Reescribe la pregunta para:\n",
    "- Usar términos más específicos relacionados con programación y ciencia de datos\n",
    "- Incluir sinónimos o términos alternativos\n",
    "- Hacer la pregunta más clara y específica\n",
    "- Mantener la intención original pero con mejor búsqueda\n",
    "\n",
    "Pregunta original: {question}\n",
    "\n",
    "Genera una pregunta reformulada que mejore la recuperación de documentos relevantes.\"\"\"\n",
    "\n",
    "WEB_SEARCH_PROMPT = \"\"\"La información en nuestra base de conocimientos no fue suficiente para responder esta pregunta sobre TechFlow Academy.\n",
    "\n",
    "Genera una respuesta útil reconociendo las limitaciones y sugiriendo próximos pasos:\n",
    "\n",
    "- Indica que la información específica no está disponible en este momento\n",
    "- Sugiere contactar directamente para información más detallada\n",
    "- Proporciona información general que sí conoces sobre TechFlow Academy\n",
    "- Mantén un tono profesional y servicial\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Contexto disponible: {documents}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3ae4e",
   "metadata": {},
   "source": [
    "## Nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcaf43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRelevance(BaseModel):\n",
    "    \"\"\"Modelo para evaluación de relevancia de documentos\"\"\"\n",
    "    decision: str  # 'yes' o 'no'\n",
    "\n",
    "class ComplexityLevel(BaseModel):\n",
    "    \"\"\"Modelo para clasificación de complejidad\"\"\"\n",
    "    complexity_level: str\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    \"\"\"Modelo para evaluación de calidad de respuesta\"\"\"\n",
    "    decision: str  # 'yes' o 'no'\n",
    "\n",
    "class QuestionRewrite(BaseModel):\n",
    "    \"\"\"Modelo para re-escritura de preguntas\"\"\"\n",
    "    rewritten_question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba266c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_messages(state: \"AdaptiveAgentState\", limit: Optional[int] = None) -> List[BaseMessage]:\n",
    "    effective_limit = limit if isinstance(limit, int) and limit > 0 else message_history_limit\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return []\n",
    "    if len(messages) <= effective_limit:\n",
    "        return messages\n",
    "    return messages[-effective_limit:]\n",
    "\n",
    "def format_docs(docs):\n",
    "        \"\"\"Format documents for RAG context\"\"\"\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#@traceable(name=\"classify_complexity\")\n",
    "def classify_complexity(state: AdaptiveAgentState):\n",
    "    \"\"\"Classify question complexity based on last human message\"\"\"\n",
    "    print(\"---CLASSIFY COMPLEXITY---\")\n",
    "    \n",
    "    complexity_prompt = PromptTemplate(template=COMPLEXITY_PROMPT, input_variables=[\"question\"])\n",
    "    complexity_classifier = (\n",
    "        complexity_prompt \n",
    "        | llm.with_structured_output(ComplexityLevel)\n",
    "    )\n",
    "    \n",
    "    # Extraer la última pregunta del usuario de los mensajes\n",
    "    last_human_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg.content\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        # Si no hay mensaje humano, usar mensaje por defecto\n",
    "        complexity_level = \"simple\"\n",
    "    else:\n",
    "        result = complexity_classifier.invoke({\"question\": last_human_message})\n",
    "        complexity_level = result.complexity_level\n",
    "    \n",
    "    print(f\"Complexity Level: {complexity_level}\")\n",
    "    # Retornar el estado actualizado con el nivel de complejidad\n",
    "    return {\n",
    "        \"complexity_level\": complexity_level,\n",
    "        \"retrieved_docs\": state.get(\"retrieved_docs\", None)\n",
    "    }\n",
    "\n",
    "#@traceable(name=\"simple_response\") \n",
    "def simple_response(state: AdaptiveAgentState):\n",
    "    \"\"\"Generate simple response using full chat history\"\"\"\n",
    "    print(\"---SIMPLE RESPONSE---\")\n",
    "\n",
    "    # Prepend a system message and pass the full history to the chat model\n",
    "    messages_for_model: List[BaseMessage] = [\n",
    "        SystemMessage(content=SIMPLE_PROMPT)\n",
    "    ] + get_last_messages(state)\n",
    "\n",
    "    response = llm.invoke(messages_for_model)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "#@traceable(name=\"rag_retrieve\")\n",
    "def rag_retrieve(state: AdaptiveAgentState):\n",
    "    \"\"\"Retrieve documents for RAG\"\"\"\n",
    "    print(\"---RAG RETRIEVE---\")\n",
    "    \n",
    "    # Obtener la pregunta (original o reformulada)\n",
    "    question = state.get(\"question\")\n",
    "    if not question:\n",
    "        # Extraer la última pregunta del usuario\n",
    "        for msg in reversed(state[\"messages\"]):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                question = msg.content\n",
    "                break\n",
    "    \n",
    "    if not question:\n",
    "        question = \"información general\"\n",
    "    \n",
    "    # Recuperar documentos\n",
    "    print(f\"Recuperando contexto de pregunta: {question}\")\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "\n",
    "    print(f\"[RAG] Docs recuperados: {len(documents)}\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        preview = doc.page_content.replace('\\n',' ')[:220]\n",
    "        print(f\"[{i}] {preview}\")\n",
    "        print(f\"    meta: {doc.metadata}\")\n",
    "\n",
    "    # Retornar estado actualizado con documentos y pregunta\n",
    "    return {\n",
    "        \"retrieved_docs\": documents,\n",
    "        \"question\": question,\n",
    "        \"max_retries\": state.get(\"max_retries\", 3),\n",
    "        \"retry_count\": state.get(\"retry_count\", 0)\n",
    "    }\n",
    "\n",
    "def grade_documents(state: AdaptiveAgentState):\n",
    "    grade_docs_prompt = PromptTemplate(template=GRADE_DOCUMENTS_PROMPT, input_variables=[\"question\", \"documents\"])\n",
    "    document_grader = (\n",
    "        grade_docs_prompt \n",
    "        | llm.with_structured_output(DocumentRelevance)\n",
    "    )\n",
    "\n",
    "    \"\"\"Grade document relevance to question\"\"\"\n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    \n",
    "    question = state.get(\"question\", \"\")\n",
    "    documents = state.get(\"retrieved_docs\", [])\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No documents to grade\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "    \n",
    "    # Formatear documentos para evaluación\n",
    "    docs_text = format_docs(documents)\n",
    "    \n",
    "    # Evaluar relevancia\n",
    "    result = document_grader.invoke({\"question\": question, \"documents\": docs_text})\n",
    "    print(f\"Document relevance: {result.decision}\")\n",
    "    \n",
    "    if result.decision == \"yes\":\n",
    "        print(\"Documents are relevant\")\n",
    "        return {\"retrieved_docs\": documents}\n",
    "    else:\n",
    "        print(\"Documents are not relevant\")\n",
    "        return {\"retrieved_docs\": []}\n",
    "\n",
    "def rag_generate(state: AdaptiveAgentState):\n",
    "    \"\"\"Generate RAG response using full chat history + context\"\"\"\n",
    "    print(\"---RAG GENERATE---\")\n",
    "\n",
    "    # Obtener documentos recuperados\n",
    "    documents = state.get(\"retrieved_docs\", [])\n",
    "    context_text = format_docs(documents) if documents else \"\"\n",
    "\n",
    "    # Prepend system with instructions and context, then pass full history\n",
    "    system_content = f\"{RAG_PROMPT}\\n\\nContexto:\\n{context_text}\"\n",
    "    messages_for_model: List[BaseMessage] = [\n",
    "        SystemMessage(content=system_content)\n",
    "    ] + get_last_messages(state)\n",
    "\n",
    "    response = llm.invoke(messages_for_model)\n",
    "    \n",
    "    # Almacenar la respuesta generada para evaluación\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"generation\": response.content\n",
    "    }\n",
    "\n",
    "def evaluate_answer(state: AdaptiveAgentState):\n",
    "    \"\"\"Evaluate if the generated answer is adequate\"\"\"\n",
    "    print(\"---EVALUATE ANSWER---\")\n",
    "\n",
    "    evaluate_answer_prompt = PromptTemplate(template=EVALUATE_ANSWER_PROMPT, input_variables=[\"question\", \"generation\", \"documents\"])\n",
    "    answer_evaluator = (\n",
    "        evaluate_answer_prompt \n",
    "        | llm.with_structured_output(AnswerEvaluation)\n",
    "    )\n",
    "    \n",
    "    question = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"retrieved_docs\", [])\n",
    "    docs_text = format_docs(documents) if documents else \"\"\n",
    "    \n",
    "    # Evaluar la respuesta\n",
    "    result = answer_evaluator.invoke({\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"documents\": docs_text\n",
    "    })\n",
    "    \n",
    "    print(f\"Answer evaluation: {result.decision}\")\n",
    "    return {\n",
    "        \"generation\": generation,\n",
    "        \"answer_evaluation\": result.decision  # Almacenar el resultado de la evaluación\n",
    "    }\n",
    "\n",
    "def rewrite_question(state: AdaptiveAgentState):\n",
    "    \"\"\"Rewrite question to improve retrieval\"\"\"\n",
    "    print(\"---REWRITE QUESTION---\")\n",
    "\n",
    "    rewrite_question_prompt = PromptTemplate(template=REWRITE_QUESTION_PROMPT, input_variables=[\"question\"])\n",
    "    question_rewriter = (\n",
    "        rewrite_question_prompt \n",
    "        | llm.with_structured_output(QuestionRewrite)\n",
    "    )\n",
    "    \n",
    "    original_question = state.get(\"question\", \"\")\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # Re-escribir la pregunta\n",
    "    result = question_rewriter.invoke({\"question\": original_question})\n",
    "    rewritten_question = result.rewritten_question\n",
    "    \n",
    "    print(f\"Question rewritten: {original_question} -> {rewritten_question}\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": rewritten_question,\n",
    "        \"retry_count\": retry_count + 1,\n",
    "        \"retrieved_docs\": []  # Limpiar documentos para nueva búsqueda\n",
    "    }\n",
    "\n",
    "def web_search_fallback(state: AdaptiveAgentState):\n",
    "    \"\"\"Fallback when RAG fails multiple times\"\"\"\n",
    "    print(\"---WEB SEARCH FALLBACK---\")\n",
    "    \n",
    "    question = state.get(\"question\", \"\")\n",
    "    documents = state.get(\"retrieved_docs\", [])\n",
    "    docs_text = format_docs(documents) if documents else \"\"\n",
    "    \n",
    "    # Generar respuesta de fallback\n",
    "    system_content = f\"{WEB_SEARCH_PROMPT}\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=system_content + \"\\n\\nPregunta: {question}\\n\\nContexto disponible: {documents}\",\n",
    "        input_variables=[\"question\", \"documents\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"question\": question, \"documents\": docs_text})\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_model_with_tools(state: AdaptiveAgentState):\n",
    "    \"\"\"Call model with tools for complex queries\"\"\"\n",
    "    print(\"---CALL MODEL WITH TOOLS---\")\n",
    "    \n",
    "    # Usar el modelo con los últimos 10 mensajes\n",
    "    response = model.invoke(get_last_messages(state))\n",
    "    \n",
    "    # Devolver solo el delta de mensajes\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "def route_by_complexity(state: AdaptiveAgentState):\n",
    "    \"\"\"Route based on complexity classification\"\"\"\n",
    "    complexity_level = state.get(\"complexity_level\", \"simple\")\n",
    "    print(f\"---ROUTE BY COMPLEXITY: {complexity_level}---\")\n",
    "    return complexity_level\n",
    "\n",
    "def decide_to_generate_or_rewrite(state: AdaptiveAgentState):\n",
    "    \"\"\"Decide whether to generate answer or rewrite question based on document relevance\"\"\"\n",
    "    documents = state.get(\"retrieved_docs\", [])\n",
    "    if documents:\n",
    "        print(\"Documents are relevant, proceeding to generate\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        retry_count = state.get(\"retry_count\", 0)\n",
    "        max_retries = state.get(\"max_retries\", 3)\n",
    "        if retry_count < max_retries:\n",
    "            print(f\"Documents not relevant, rewriting question (attempt {retry_count + 1}/{max_retries})\")\n",
    "            return \"rewrite\"\n",
    "        else:\n",
    "            print(f\"Max retries reached ({max_retries}), using fallback\")\n",
    "            return \"fallback\"\n",
    "\n",
    "def decide_to_finish_or_rewrite(state: AdaptiveAgentState):\n",
    "    \"\"\"Decide whether to finish or rewrite based on answer evaluation\"\"\"\n",
    "    answer_evaluation = state.get(\"answer_evaluation\", \"yes\")\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    print(f\"Answer evaluation decision: {answer_evaluation}\")\n",
    "    \n",
    "    # Si la respuesta es buena, terminar\n",
    "    if answer_evaluation == \"yes\":\n",
    "        print(\"Answer is adequate, finishing\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    # Si la respuesta no es buena y aún hay reintentos disponibles\n",
    "    if retry_count < max_retries:\n",
    "        print(f\"Answer needs improvement, rewriting question (attempt {retry_count + 1}/{max_retries})\")\n",
    "        return \"rewrite\"\n",
    "    \n",
    "    # Si se alcanzó el máximo de reintentos, terminar de todas formas\n",
    "    print(f\"Max retries reached ({max_retries}), finishing with current answer\")\n",
    "    return \"finish\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed4c25",
   "metadata": {},
   "source": [
    "## Creación del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# MemorySaver para almacenar el estado de la conversación en el hilo actual\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(AdaptiveAgentState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"classify_complexity\", classify_complexity)\n",
    "builder.add_node(\"simple_response\", simple_response)\n",
    "\n",
    "# RAG adaptativo nodes\n",
    "builder.add_node(\"rag_retrieve\", rag_retrieve)\n",
    "builder.add_node(\"grade_documents\", grade_documents)\n",
    "builder.add_node(\"rag_generate\", rag_generate)\n",
    "builder.add_node(\"evaluate_answer\", evaluate_answer)\n",
    "builder.add_node(\"rewrite_question\", rewrite_question)\n",
    "builder.add_node(\"web_search_fallback\", web_search_fallback)\n",
    "\n",
    "# Tools nodes\n",
    "builder.add_node(\"call_model_with_tools\", call_model_with_tools)\n",
    "builder.add_node(\n",
    "    \"tools\",\n",
    "    ToolNode([registrar_cliente, contar_registros, get_current_date, get_program_price])\n",
    ")\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"classify_complexity\")\n",
    "\n",
    "# Conditional routing from complexity classifier\n",
    "builder.add_conditional_edges(\n",
    "    \"classify_complexity\",\n",
    "    route_by_complexity,\n",
    "    {\n",
    "        \"simple\": \"simple_response\",\n",
    "        \"rag\": \"rag_retrieve\", \n",
    "        \"tools\": \"call_model_with_tools\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simple path\n",
    "builder.add_edge(\"simple_response\", END)\n",
    "\n",
    "# RAG adaptativo path\n",
    "builder.add_edge(\"rag_retrieve\", \"grade_documents\")\n",
    "builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate_or_rewrite,\n",
    "    {\n",
    "        \"generate\": \"rag_generate\",\n",
    "        \"rewrite\": \"rewrite_question\",\n",
    "        \"fallback\": \"web_search_fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Re-write loop\n",
    "builder.add_edge(\"rewrite_question\", \"rag_retrieve\")\n",
    "\n",
    "# Generate and evaluate\n",
    "builder.add_edge(\"rag_generate\", \"evaluate_answer\")\n",
    "builder.add_conditional_edges(\n",
    "    \"evaluate_answer\",\n",
    "    decide_to_finish_or_rewrite,\n",
    "    {\n",
    "        \"finish\": END,\n",
    "        #\"finish\": \"classify_complexity\",\n",
    "        \"rewrite\": \"rewrite_question\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fallback path\n",
    "builder.add_edge(\"web_search_fallback\", END)\n",
    "\n",
    "# Tools path\n",
    "builder.add_conditional_edges(\n",
    "    \"call_model_with_tools\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END}\n",
    ")\n",
    "builder.add_edge(\"tools\", \"call_model_with_tools\")\n",
    "\n",
    "# Se compila el grafo con nombre estable para asegurar que se recupere el estado tras reinicios\n",
    "graph_name = os.getenv(\"GRAPH_NAME\", \"chatbot_graph_v1\")\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=within_thread_memory,\n",
    "    name=graph_name,\n",
    ")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e7276",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@traceable(name=\"process_message\", metadata={\"component\": \"adaptive_rag_chatbot\"})\n",
    "def process_message(message: str, user_id: str) -> Dict[str, Any]:\n",
    "    config = {\"configurable\": {\"thread_id\": user_id, \"user_id\": user_id, \"checkpoint_ns\": graph_name}}\n",
    "    input_messages = [HumanMessage(content=message)]\n",
    "\n",
    "    # Inicializar el estado con todos los campos requeridos\n",
    "    initial_state = {\n",
    "        \"messages\": input_messages,\n",
    "        \"complexity_level\": None,\n",
    "        \"retrieved_docs\": None,\n",
    "        \"question\": None,\n",
    "        \"generation\": None,\n",
    "        \"answer_evaluation\": None,\n",
    "        \"max_retries\": 3,\n",
    "        \"retry_count\": 0\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(initial_state, config)\n",
    "    ai_message = result[\"messages\"][-1]\n",
    "\n",
    "    return ai_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcacd8",
   "metadata": {},
   "source": [
    "## Prueba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message(\"que programas hay?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b481a",
   "metadata": {},
   "source": [
    "## Prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message(\"cual es el costo del programa Data Engineer?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410702c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc75f3",
   "metadata": {},
   "source": [
    "## Prueba 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e497efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message(\"Que programas hay y cual es el costo de cada uno?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509afdcd",
   "metadata": {},
   "source": [
    "# RAG como Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92515012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_techflow_academy_info\", # nombre de la tool\n",
    "    \"Busca y devuelve información sobre Programas, cursos, docentes, empresas aliadas, preguntas frecuentes, testimonio de graduados, matriculas y reglamentos sobre el instituto Techflow Academy\", # descripcion de la tool\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    retriever_tool,\n",
    "    registrar_cliente,\n",
    "    contar_registros,\n",
    "    get_current_date,\n",
    "    get_program_price\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efb255",
   "metadata": {},
   "source": [
    "```python\n",
    "class AdaptiveAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Estado personalizado que incluye mensajes y metadatos del RAG adaptativo\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    complexity_level: Optional[str]\n",
    "    retrieved_docs: Optional[List[Any]]\n",
    "    question: Optional[str]  # Pregunta actual (original o reformulada)\n",
    "    generation: Optional[str]  # Respuesta generada\n",
    "    answer_evaluation: Optional[str]  # Resultado de evaluación de la respuesta ('yes'/'no')\n",
    "    max_retries: Optional[int]  # Máximo número de reintentos\n",
    "    retry_count: Optional[int]  # Contador de reintentos\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo nodo inicial\n",
    "def agent(state: AdaptiveAgentState):\n",
    "    \"\"\"\n",
    "    Invoca al modelo del agente para generar una respuesta basada en el estado actual. Dada\n",
    "    la pregunta, decidirá si recuperar usando la herramienta retriever o simplemente terminar.\n",
    "\n",
    "    Args:\n",
    "        state (messages): El estado actual\n",
    "\n",
    "    Returns:\n",
    "        dict: El estado actual con la respuesta del agente agregada a los mensajes\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    messages.append(HumanMessage(content=\"Responde la pregunta del usuario basado en la cadena de mensajes y la peticion que te hizo\"))\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5604f9",
   "metadata": {},
   "source": [
    "## Nuevo grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "def validar_rag(state: \"AdaptiveAgentState\") -> Literal[\"generate\", \"agent\"]:\n",
    "    \"\"\"\n",
    "    Valida si se usó la tool del retriever (RAG) para decidir el siguiente nodo.\n",
    "    \n",
    "    Si se ejecutó la tool 'retrieve_techflow_academy_info', va al nodo 'generate'.\n",
    "    Si se ejecutaron otras tools, regresa al nodo 'agent'.\n",
    "    \"\"\"\n",
    "    print(\"---VALIDAR RAG---\")\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Buscar el último mensaje AI que tenga tool_calls\n",
    "    for message in reversed(messages):\n",
    "        if isinstance(message, AIMessage) and hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            # Verificar si alguna de las tool_calls es la del retriever\n",
    "            for tool_call in message.tool_calls:\n",
    "                tool_name = tool_call.get(\"name\", \"\")\n",
    "                print(f\"Tool ejecutada: {tool_name}\")\n",
    "                \n",
    "                if tool_name == \"retrieve_techflow_academy_info\":\n",
    "                    print(\"Se detectó tool del retriever RAG -> ir a 'generate'\")\n",
    "                    return \"generate\"\n",
    "            \n",
    "            # Si llegamos aquí, se ejecutaron otras tools pero no el retriever\n",
    "            print(\"Se ejecutaron otras tools (no RAG) -> regresar a 'agent'\")\n",
    "            return \"agent\"\n",
    "    \n",
    "    # Si no se encontraron tool_calls, por defecto regresar al agent\n",
    "    print(\"No se encontraron tool_calls -> regresar a 'agent'\")\n",
    "    return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: AdaptiveAgentState):\n",
    "    \"\"\"\n",
    "    Genera una respuesta basada en el contexto RAG recuperado\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Buscar el último ToolMessage que contenga el contexto del RAG\n",
    "    rag_context = \"\"\n",
    "    for message in reversed(messages):\n",
    "        if hasattr(message, 'content') and isinstance(message.content, str):\n",
    "            # Los resultados del retriever vienen en ToolMessage\n",
    "            if \"Documento:\" in message.content or any(keyword in message.content.lower() for keyword in [\"techflow\", \"programa\", \"curso\"]):\n",
    "                rag_context = message.content\n",
    "                print(f\"Contexto RAG encontrado: {rag_context[:200]}...\")\n",
    "                break\n",
    "    \n",
    "    # Si no encontramos contexto específico, usar el último mensaje de tool\n",
    "    if not rag_context:\n",
    "        for message in reversed(messages):\n",
    "            if hasattr(message, 'content') and len(str(message.content)) > 50:\n",
    "                rag_context = str(message.content)\n",
    "                print(f\"Usando último contenido como contexto: {rag_context[:200]}...\")\n",
    "                break\n",
    "    \n",
    "    # Crear prompt para generar respuesta basada en el contexto RAG\n",
    "    system_prompt = f\"\"\"Eres un asistente especializado de TechFlow Academy.\n",
    "\n",
    "Basándote en la siguiente información recuperada, genera una respuesta clara y útil para el usuario.\n",
    "\n",
    "Contexto RAG:\n",
    "{rag_context}\n",
    "\n",
    "Instrucciones:\n",
    "- Responde de manera clara y estructurada\n",
    "- Usa solo la información proporcionada en el contexto\n",
    "- Si la información es limitada, indícalo y sugiere formas de obtener más detalles\n",
    "- Mantén un tono profesional y amigable\"\"\"\n",
    "\n",
    "    # Obtener la pregunta original del usuario\n",
    "    user_question = \"\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            user_question = message.content\n",
    "    \n",
    "    # Crear el mensaje para el modelo\n",
    "    messages_for_model = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_question)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages_for_model)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# MemorySaver para almacenar el estado de la conversación en el hilo actual\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(AdaptiveAgentState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"agent\", agent)\n",
    "execute = ToolNode(tools)\n",
    "builder.add_node(\"execute\", execute)  # retrieval\n",
    "builder.add_node(\"generate\", generate)  # genera respuestas basadas en RAG\n",
    "\n",
    "# Flujo: START -> agent -> execute -> (generate o agent)\n",
    "builder.add_edge(START, \"agent\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"execute\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"execute\",\n",
    "    validar_rag,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"agent\": \"agent\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# El nodo generate regresa al agent para continuar la conversación\n",
    "builder.add_edge(\"generate\", \"agent\")\n",
    "\n",
    "graph_name = os.getenv(\"GRAPH_NAME\", \"chatbot_graph_v1\")\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=within_thread_memory,\n",
    "    name=graph_name,\n",
    ")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar mensajes (similar a la anterior)\n",
    "@traceable(name=\"process_message\", metadata={\"component\": \"adaptive_rag_chatbot\"})\n",
    "def process_message_v2(message: str, user_id: str) -> str:\n",
    "    config = {\"configurable\": {\"thread_id\": user_id, \"user_id\": user_id, \"checkpoint_ns\": graph_name}}\n",
    "    input_messages = [HumanMessage(content=message)]\n",
    "\n",
    "    # Inicializar el estado\n",
    "    initial_state = {\n",
    "        \"messages\": input_messages,\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(initial_state, config)\n",
    "    ai_message = result[\"messages\"][-1]\n",
    "\n",
    "    return ai_message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d422f0",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabb977",
   "metadata": {},
   "source": [
    "## Prueba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0541eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message_v2(\"que programas hay?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee18d61",
   "metadata": {},
   "source": [
    "## Prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badeef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message_v2(\"cual es el costo del programa Data Engineer?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9135a1",
   "metadata": {},
   "source": [
    "## Prueba 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c80913",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = process_message_v2(\"Que programas hay y cual es el costo de cada uno?\", \"ernesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eea698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
